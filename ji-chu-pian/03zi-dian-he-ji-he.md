# 字典和集合

## 泛映射类型

`collections.abc` 模块中有 `Mapping` 和 `MutableMapping` 这两个抽象 基类，它们的作用是为 `dict` 和其他类似的类型定义形式接口（在 **Python2.6** 到**Python3.2**的版本中，这些类还不属于 `collections.abc` 模块，而是隶属于 `collections` 模块）。见图 03-1。

![ZyQnZn.png](https://s2.ax1x.com/2019/07/09/ZyQnZn.png)

> 图03-1：`collections.abc`中的`MutableMapping`和它的超类的`UML`类图（箭头从子类指向超类，抽象类和抽象方法的名称以斜体显示）

然而，非抽象映射类型一般不会直接继承这些抽象基类，它们会直接对 `dict` 或是 `collections.User.Dict` 进行扩展。这些抽象基类的主要作用是作为形式化的文档，它们定义了构建一个映射类型所需要的最基本的接口。然后它们还可以跟 `isinstance` 一起被用来判定某个数据是不是广义上的映射类型：

```py
>>> my_dict = {} 
>>> isinstance(my_dict, abc.Mapping) 
True
```

这里用 `isinstance` 而不是 `type` 来检查某个参数是否为 `dict` 类型， 因为这个参数有可能不是 `dict`，而是一个比较另类的映射类型。

标准库里的所有映射类型都是利用 `dict` 来实现的，因此它们有个共同的限制，即只有可散列的数据类型才能用作这些映射里的键（只有键有这个要求，值并不需要是可散列的数据类型）。

**什么是可散列的数据类型**?

* 如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现 `__hash__()` 方法。另外可散列对象还要有 `__qe__()` 方法，这样才能跟其他键做比较。如果两个可散列对象是相等的，那么它们的散列值一定是一样的

* 原子不可变数据类型（`str`、`bytes` 和数值类型）都是可散列类 型，`frozenset` 也是可散列的，因为根据其定义，`frozenset` 里只能容纳可散列类型。元组的话，只有当一个元组包含的所有元素都是可散列类型的情况下，它才是可散列的。来看下面的元组 `tt`、`tl` 和 `tf`：

  ```py
   >>> tt = (1, 2, (30, 40)) 
   >>> hash(tt) 
   8027212646858338501 
   >>> tl = (1, 2, [30, 40]) 
   >>> hash(tl) 
   Traceback (most recent call last):
      File "<stdin>", line 1, in <module> 
   TypeError: unhashable type: 'list' 
   >>> tf = (1, 2, frozenset([30, 40])) 
   >>> hash(tf) 
   -4118419923444501110
  ```

* 一般来讲用户自定义的类型的对象都是可散列的，散列值就是它们的 `id()` 函数的返回值，所以所有这些对象在比较的时候都是不相等的。如果一个对象实现了 `__eq__` 方法，并且在方法中用到了这个对象的内部状态的话，那么只有当所有这些内部状态都是不可变的情况下，这个对象才是可散列的。

创建字典的不同方式：

```py
>>> a = dict(one=1, two=2, three=3) 
>>> b = {'one': 1, 'two': 2, 'three': 3} 
>>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3])) 
>>> d = dict([('two', 2), ('one', 1), ('three', 3)]) 
>>> e = dict({'three': 3, 'one': 1, 'two': 2}) 
>>> a == b == c == d == e 
True
```

## 字典推导

字典推导的应用：

```py
>>> DIAL_CODES = [                ➊ 
...         (86, 'China'), 
...         (91, 'India'), 
...         (1, 'United States'), 
...         (62, 'Indonesia'), 
...         (55, 'Brazil'), 
...         (92, 'Pakistan'), 
...         (880, 'Bangladesh'), 
...         (234, 'Nigeria'), 
...         (7, 'Russia'), 
...         (81, 'Japan'), 
...     ] 
>>> country_code = {country: code for code, country in DIAL_CODES}  ➋
>>> country_code 
{'China': 86, 'India': 91, 'Bangladesh': 880, 'United States': 1, 
'Pakistan': 92, 'Japan': 81, 'Russia': 7, 'Brazil': 55, 'Nigeria': 
234, 'Indonesia': 62} 
>>> {code: country.upper() for country, code in country_code.items()  ➌ 
...   if code < 66} 
{1: 'UNITED STATES', 55: 'BRAZIL', 62: 'INDONESIA', 7: 'RUSSIA'}
```

❶ 一个承载成对数据的列表，它可以直接用在字典的构造方法中。 

❷ 这里把配好对的数据左右换了下，国家名是键，区域码是值。 

❸ 跟上面相反，用区域码作为键，国家名称转换为大写，并且过滤掉区域码大于或等于`66`的地区。 

## 映射的弹性键查询 

有时候为了方便起见，就算某个键在映射里不存在，我们也希望在通过 这个键读取值的时候能得到一个默认值。有两个途径能帮我们达到这个目的，一个是通过 `defaultdict` 这个类型而不是普通的 `dict`，另一个 是给自己定义一个 `dict` 的子类，然后在子类中实现`__missing__`方法。

### `defaultdict`：处理找不到的键的一个选择 

在用户创建 `defaultdict` 对象的时候，就需要给它配置 一个为找不到的键创造默认值的方法。

具体而言，在实例化一个 `defaultdict` 的时候，需要给构造方法提供一个可调用对象，这个可调用对象会在 `__getitem__` 碰到找不到的键的时候被调用，让 `__getitem__` 返回某种默认值。

比如，我们新建了这样一个字典：`dd = defaultdict(list)`，如果键 `'new-key'` 在 `dd` 中还不存在的话，表达式 `dd['new-key']` 会按照以下的步骤来行事。 

 * 调用 `list()` 来建立一个新列表。 
 
 * 把这个新列表作为值，`'new-key'` 作为它的键，放到 `dd` 中。
 
 * 返回这个列表的引用。 

### 特殊方法`__missing__ `

所有的映射类型在处理找不到的键的时候，都会牵扯到 `__missing__` 方法。这也是这个方法称作“missing”的原因。虽然基类 `dict`并没有定义这个方法，但是 `dict` 是知道有这么个东西存在的。也就是说，如果 有一个类继承了 `dict`，然后这个继承类提供了 `__missing__` 方法，那么在 `__getitem__` 碰到找不到的键的时候，**Python**就会自动调用它， 而不是抛出一个 `KeyError` 异常。

> `__missing__` 方法只会被 `__getitem__` 调用（比如在表达式 `d[k]` 中）。提供 `__missing_`_ 方法对 `get` 或者 `__contains__`（in 运算符会用到这个方法）这些方法的使用没有影响。

StrKeyDict0 在查询的时候把非字符串的键转换为字符串: 

> 如果要自定义一个映射类型，更合适的策略其实是继承 `collections.UserDict` 类。这里我们从 `dict` 继承，只是为了演示 `__missing__` 是如何被 `dict.__getitem__` 调用的。 

```py
class StrKeyDict0(dict):  ➊

   def __missing__(self, key):
       if isinstance(key, str):  ➋
           raise KeyError(key)
       return self[str(key)]  ➌
       
   def get(self, key, default=None):
       try:
           return self[key]  ➍
       except KeyError:
           return default  ➎ 
           
   def __contains__(self, key):
       return key in self.keys() or str(key) in self.keys()  ➏ 
```

❶ `StrKeyDict0` 继承了 `dict`。 

❷ 如果找不到的键本身就是字符串，那就抛出 `KeyError` 异常。 

❸ 如果找不到的键不是字符串，那么把它转换成字符串再进行查找。 

❹ `get` 方法把查找工作用 `self[key]` 的形式委托给 `__getitem__`，这 样在宣布查找失败之前，还能通过 `__missing__` 再给某个键一个机 会。 

❺ 如果抛出 `KeyError`，那么说明 `__missing__` 也失败了，于是返回 `default`。 

❻ 先按照传入键的原本的值来查找（我们的映射类型中可能含有非字 符串的键），如果没找到，再用 `str()` 方法把键转换成字符串再查找一次。 

下面来看看为什么 `isinstance(key, str)` 测试在上面的 `__missing__` 中是必需的。 

* 如果没有这个测试，只要 `str(k)` 返回的是一个存在的键，那么 `__missing__` 方法是没问题的，不管是字符串键还是非字符串键，它都能正常运行。但是如果 `str(k)` 不是一个存在的键，代码就会陷入无限递归。这是因为 `__missing__` 的最后一行中的 `self[str(key)]` 会调用 `__getitem__`，而这个 `str(key)` 又不存在，于是 `__missing__` 又会被调用。

为了保持一致性，`__contains__` 方法在这里也是必需的。这是因为 `k in d` 这个操作会调用它，但是我们从 `dict` 继承到的 `__contains__` 方法不会在找不到键的时候调用 `__missing__` 方法。`__contains__` 里还有个细节，就是我们这里没有用更具 **Python** 风格的方式——`k in my_dict`——来检查键是否存在，因为那也会导致 `__contains__` 被递归调用。为了避免这一情况，这里采取了更显式的方法，直接在这个 `self.keys()` 里查询。

> 像 `k in my_dict.keys()` 这种操作在 **Python3** 中是很快 的，而且即便映射类型对象很庞大也没关系。这是因为 `dict.keys()` 的返回值是一个“视图”。视图就像一个集合，而且跟 字典类似的是，在视图里查找一个元素的速度很快。

## 字典的变种 

* `collections.OrderedDict`

    这个类型在添加键的时候会保持顺序，因此键的迭代次序总是一致的。`OrderedDict` 的 `popitem` 方法默认删除并返回的是字典里的最后一个元素，但是如果像 `my_odict.popitem(last=False)` 这样调用它，那么它删除并返回第一个被添加进去的元素。 

* `collections.ChainMap`

    该类型可以容纳数个不同的映射对象，然后在进行键查找操作的时 候，这些对象会被当作一个整体被逐个查找，直到键被找到为止。这个功能在给有嵌套作用域的语言做解释器的时候很有用，可以用一个映射 对象来代表一个作用域的上下文。

* `collections.Counter`

    这个映射类型会给键准备一个整数计数器。每次更新一个键的时候都会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集来用——多重集合就是集合里的元素可以出现不止一次。`Counter` 实现了 `+` 和 `-` 运算符用来合并记录，还有像 `most_common([n])` 这类很有用的方法。`ost_common([n])` 会按照次序返回映射里最常见的 `n` 个键和它们的计数

    下面的小例子利用 Counter 来计算单词中各个字母出现的次数： 
    
    ```py
    >>> ct = collections.Counter('abracadabra') 
    >>> ct 
    Counter({'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1}) 
    >>> ct.update('aaaaazzz') 
    >>> ct 
    Counter({'a': 10, 'z': 3, 'b': 2, 'r': 2, 'c': 1, 'd': 1}) 
    >>> ct.most_common(2) 
    [('a', 10), ('z', 3)] 
    ```
* `colllections.UserDict`

    这个类其实就是把标准 `dict` 用纯 **Python** 又实现了一遍。 
    
## 子类化UserDict 

就创造自定义映射类型来说，以 `UserDict` 为基类，总比以普通的 `dict` 为基类要来得方便。 

而更倾向于从 `UserDict` 而不是从 `dict` 继承的主要原因是，后者有时会在某些方法的实现上走一些捷径，导致我们不得不在它的子类中重写这些方法，但是 `UserDict` 就不会带来这些问题。

另外一个值得注意的地方是，`UserDict` 并不是 `dict` 的子类，但是 `UserDict` 有一个叫作 `data` 的属性，是 `dict` 的实例，这个属性实际上 是 `UserDict` 最终存储数据的地方。这样做的好处是，，`UserDict` 的子类就能在实现 `__setitem__` 的时候避免不必要的递归，也可以让 `__contains__` 里的代码更简洁。


```py
import collections 

class StrKeyDict(collections.UserDict):  ➊

    def __missing__(self, key):  ➋
        if isinstance(key, str):
            raise KeyError(key)
        return self[str(key)]
        
    def __contains__(self, key):
        return str(key) in self.data  ➌
        
    def __setitem__(self, key, item):
        self.data[str(key)] = item  ➍ 
```

❶ `StrKeyDict` 是对 `UserDict` 的扩展。 

❷ `__missing__` 跟前面示例里的一模一样。 

❸ `__contains__` 则更简洁些。这里可以放心假设所有已经存储的键都是字符串。因此，只要在`self.data`上查询就好了，并不需要像 `StrKeyDict0` 那样去麻烦 `self.keys()`。 

❹ `__setitem__` 会把所有的键都转换成字符串。由于把具体的实现委托给了 `self.data` 属性，这个方法写起来也不难。 

因为 `UserDict` 继承的是 `MutableMapping`，所以 `StrKeyDict` 里剩下 的那些映射类型的方法都是从 `UserDict`、`MutableMapping` 和 `Mapping` 这些超类继承而来的。特别是最后的 `Mapping` 类，它虽然是 一个抽象基类（`ABC`），但它却提供了好几个实用的方法。以下两个方法值得关注。 

    * `MutableMapping.update`
    
        这个方法不但可以为我们所直接利用，它还用在 `__init__` 里，让构造方法可以利用传入的各种参数（其他映射类型、元素是 (k`ey, value`) 对的可迭代对象和键值参数）来新建实例。因为这个方法在背后是用 `self[key] = value` 来添加新值的，所以它其实是在使用我们的 `__setitem__` 方法。 
    * `Mapping.get`
    
        在 `StrKeyDict0`最之前中，我们不得不改写 `get` 方法，好让它的表现跟 `__getitem__` 一致。而在现在的示例中就没这个必要了，因为它继承了 `Mapping.get` 方法 而 **Python** 的源码 显示，这个方法的实现方式跟 `StrKeyDict0.get` 是一模一样的。

## 不可变映射类型 

标准库里所有的映射类型都是可变的，但有时候你会有这样的需求，比如不能让用户错误地修改某个映射。

从 **Python3.3** 开始，`types`模块中引入了一个封装类名叫 `MappingProxyType`。如果给这个类一个映射，它会返回一个只读的映射视图。虽然是个只读视图，但是它是动态的。这意味着如果对原映射做出了改动，我们通过这个视图可以观察到，但是无法通过这个视图对原映射做出修改。

用 `MappingProxyType` 来获取字典的只读实例 `mappingproxy` ：

```py
>>> from types import MappingProxyType 
>>> d = {1:'A'} 
>>> d_proxy = MappingProxyType(d) 
>>> d_proxy 
mappingproxy({1: 'A'}) 
>>> d_proxy[1]  ➊ 
'A' 
>>> d_proxy[2] = 'x'  ➋ 
Traceback (most recent call last):
    File "<stdin>", line 1, in <module> 
TypeError: 'mappingproxy' object does not support item assignment 
>>> d[2] = 'B' 
>>> d_proxy  ➌ 
mappingproxy({1: 'A', 2: 'B'}) 
>>> d_proxy[2] 
'B'
```

➊ `d` 中的内容可以通过 `d_proxy` 看到。 

➋ 但是通过 `d_proxy` 并不能做任何修改。

➌ `d_proxy` 是动态的，也就是说对 `d` 所做的任何改动都会反馈到它上面。 

## 集合论 

“集”这个概念在 **Python** 中算是比较年轻的，同时它的使用率也比较 低。`set` 和它的不可变的姊妹类型 `frozenset` 直到 **Python2.3** 才首次以模块的形式出现，然后在 **Python2.6** 中它们升级成为内置类型。

集合的本质是许多唯一对象的聚集。因此，集合可以用于去重：

```py
>>> l = ['spam', 'spam', 'eggs', 'spam'] 
>>> set(l) 
{'eggs', 'spam'} 
>>> list(set(l)) 
['eggs', 'spam'] 
```

集合中的元素必须是可散列的，`set` 类型本身是不可散列的，但是 `frozenset` 可以。因此可以创建一个包含不同 frozenset 的 set

除了保证唯一性，集合还实现了很多基础的中缀运算符。给定两个集合 `a` 和 `b`，`a | b` 返回的是它们的合集，`a & b` 得到的是交集，而 `a - b` 得到的是差集。合理地利用这些操作，不仅能够让代码的行数变少，还能减少 **Python** 程序的运行时间。这样做同时也是为了让代码更易读，从而更容易判断程序的正确性，因为利用这些运算符可以省去不必要的循 环和逻辑操作。 

示例03 - 2， needles 的元素在 haystack 里出现的次数，两个变量都是 set 类型 :

```py
found = len(needles & haystack) 
```

示例03 - 3， 如果不使用交集操作的话，代码可能就变成了这样：

```py
found = 0
for n in needles:
    if n in haystack:
        found += 1
```

示例 03-2 比示例 03-3 的速度要快一些；另一方面，示例 03-3 可以用在 任何可迭代对象 `needles` 和 `haystack` 上，而示例 03-2 则要求两个对象都是集合。话再说回来，就算手头没有集合，我们也可以随时建立集合，如示例 03-4 所示。 

示例 03-4 `needles` 的元素在 `haystack` 里出现的次数，这次的代码可以用在任何可迭代对象上 

```py
found = len(set(needles) & set(haystack)) 

# 另一种写法： 
found = len(set(needles).intersection(haystack)) 
```

示例 03-4 里的这种写法会牵扯到把对象转化为集合的成本，不过如果 `needles` 或者是 `haystack` 中任意一个对象已经是集合，那么示例 03-4 的方案可能就比示例 03-3 里的要更高效。 

### 集合字面量 

除空集之外，集合的字面量——`{1}、{1, 2}`，等等——看起来跟它的 数学形式一模一样。如果是空集，那么必须写成 `set()` 的形式。

> 不要忘了，如果要创建一个空集，你必须用不带任何参数的构造方法 `set()`。如果只是写成 `{}` 的形式，跟以前一样，你创建的其实是个空字典。 

在 **Python3** 里面，除了空集，集合的字符串表示形式总是以 `{...}` 的形式出现。 

```py
>>> s = {1} 
>>> type(s) 
<class 'set'> 
>>> s 
{1} 
>>> s.pop() 
1 
>>> s 
set()
```

像 `{1, 2, 3}` 这种字面量句法相比于构造方法（`set([1, 2, 3])`）要 更快且更易读。后者的速度要慢一些，因为 **Python** 必须先从 `set` 这个名字来查询构造方法，然后新建一个列表，最后再把这个列表传入到构造方法里。但是如果是像 `{1, 2, 3}` 这样的字面量，**Python** 会利用一个专门的叫作 `BUILD_SET` 的字节码来创建集合。 

用 `dis.dis`（反汇编函数）来看看两个方法的字节码的不同： 

```py
>>> from dis import dis 
>>> dis('{1}')                                  ➊
  1           0 LOAD_CONST             0 (1)
              3 BUILD_SET              1        ➋
              6 RETURN_VALUE 
 >>> dis('set([1])')                             ➌
  1           0 LOAD_NAME              0 (set)  ➍
              3 LOAD_CONST             0 (1)
              6 BUILD_LIST             
              19 CALL_FUNCTION          1 (1 positional, 0 keyword pair)
              12 RETURN_VALUE 
```

➊ 检查 `{1}` 字面量背后的字节码。 

➋ 特殊的字节码 `BUILD_SET` 几乎完成了所有的工作。 

➌ `set([1])` 的字节码。 

➍ `3` 种不同的操作代替了上面的 `BUILD_SET`：`LOAD_NAME`、`BUILD_LIST` 和 `CALL_FUNCTION`。 

由于 **Python** 里没有针对 `frozenset` 的特殊字面量句法，我们只能采用构造方法。**Python3** 里 `frozenset` 的标准字符串表示形式看起来就像构造方法调用一样。来看这段控制台对话： 

```py
>>> frozenset(range(10)) 
frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}) 
```

### 集合推导 

新建一个 `Latin-1` 字符集合，该集合里的每个字符的 `Unicode` 名字里都有“`SIGN`”这个单词:

```py
>>> from unicodedata import name  ➊ 
>>> {chr(i) for i in range(32, 256) if 'SIGN' in name(chr(i),'')}  ➋ 
{'§', '=', '¢', '#', '¤', '<', '¥', 'μ', '×', '$', '¶', '£', '©',
'°', '+', '÷', '±', '>', '¬', '®', '%'} 
```

➊ 从 `unicodedata` 模块里导入 `name` 函数，用以获取字符的名字。 

➋ 把编码在 `32~255` 之间的字符的名字里有“`SIGN`”单词的挑出来，放到一个集合里。 

### 集合的操作 

图 03-2 列出了可变和不可变集合所拥有的方法的概况，其中不少是运算符重载的特殊方法。

![ZyIs1I.png](https://s2.ax1x.com/2019/07/09/ZyIs1I.png)

> 图 03-2：`collections.abc` 中，`MutableSet` 和它的超类的 `UML` 类图 （箭头从子类指向超类，抽象类和抽象方法的名称以斜体显示，其中省略了反向运算符方法）

## dict和set的背后 

想要理解 **Python** 里字典和集合类型的长处和弱点，它们背后的散列表是绕不开的一环。 

### 一个关于效率的实验 

所有的 **Python** 程序员都从经验中得出结论，认为字典和集合的速度是非常快的。接下来我们要通过可控的实验来证实这一点。 

为了对比容器的大小对 `dict`、`set` 或 `list` 的 `in` 运算符效率的影响， 我创建了一个有 `1000` 万个双精度浮点数的数组，名叫 `haystack`。另外还有一个包含了 `1000` 个浮点数的 `needles` 数组，其中 `500` 个数字是从 `haystack` 里挑出来的，另外 `500` 个肯定不在 `haystack` 里。

作为 `dict` 测试的基准，我用 `dict.fromkeys()` 来建立了一个含有`1000` 个浮点数的名叫 `haystack` 的字典，并用 `timeit` 模块测试示例 03-6 里这段代码运行所需要的时间。 

示例 03-6 在 `haystack` 里查找 `needles` 的元素，并计算找到的元素的个数:

```py
found = 0 
for n in needles:
    if n in haystack:
        found += 1 
```

然后这段基准测试重复了 `4` 次，每次都把 `haystack` 的大小变成了上一 次的 `10` 倍，直到里面有 `1000` 万个元素。最后这些测试的结果列在了表 3-5 中。 

> 表3-5：用in运算符在5个不同大小的haystack字典里搜索1000个元 素所需要的时间。代码运行在一个Core i7笔记本上，Python版本是 3.4.0（测试计算的是示例3-14里循环的运行时间） 

    ![ZyTOfJ.png](https://s2.ax1x.com/2019/07/09/ZyTOfJ.png)
    
也就是说，在我的笔记本上从 `1000` 个字典键里搜索 `1000` 个浮点数所需 的时间是 `0.000202` 秒，把同样的搜索在含有 `10 000 000` 个元素的字典里进行一遍，只需要 `0.000337` 秒。换句话说，在一个有 `1000` 万个键的 字典里查找 `1000` 个数，花在每个数上的时间不过是 `0.337` 微秒——没错，相当于平均每个数差不多三分之一微秒。 

作为对比，我把 `haystack` 换成了 `set` 和 `list` 类型，重复了同样的增长大小的实验。对于 `set`，除了上面的那个循环的运行时间，我还测量了示例 03-7 那行代码，这段代码也计算了 `needles` 中出现在 `haystack` 中的元素的个数。 

示例 03-7 利用交集来计算 `needles` 中出现在 `haystack` 中的元素的个数:

```py
found = len(needles & haystack) 
```

表 3-6 列出了所有测试的结果。最快的时间来自“集合交集花费时间”这 一列，这一列的结果是示例 03-7 中利用集合 & 操作的代码的效果。不出所料的是，最糟糕的表现来自“列表花费时间”这一列。由于列表的背 后没有散列表来支持 in 运算符，每次搜索都需要扫描一次完整的列表，导致所需的时间跟据 `haystack` 的大小呈线性增长。 

> 表3-6：在5个不同大小的haystack里搜索1000个元素所需的时 间，haystack分别以字典、集合和列表的形式出现。测试环境是一 个有Core i7处理器的笔记本，Python版本是3.4.0（测试所测量的代码 是示例3-14中的循环和示例3-15的集合&操作） 

    ![Zy7Nn0.png](https://s2.ax1x.com/2019/07/09/Zy7Nn0.png)
    
### 字典中的散列表   
    
散列表其实是一个稀疏数组（总是有空白元素的数组称为稀疏数组）。在一般的数据结构教材中，散列表里的单元通常叫作表元（`bucket`）。 在 `dict` 的散列表当中，每个键值对都占用一个表元，每个表元都有两个部分，一个是对键的引用，另一个是对值的引用。因为所有表元的大小一致，所以可以通过偏移量来读取某个表元。     
    
因为 **Python** 会设法保证大概还有三分之一的表元是空的，所以在快要达到这个阈值的时候，原有的散列表会被复制到一个更大的空间里面。 
    
如果要把一个对象放入散列表，那么首先要计算这个元素键的散列值。 **Python** 中可以用 `hash()` 方法来做这件事情，接下来会介绍这一点。     
    
* 散列值和相等性

    内置的 `hash()` 方法可以用于所有的内置类型对象。如果是自定义对象调用 `hash()` 的话，实际上运行的是自定义的 `__hash__`。如果两个对象在比较的时候是相等的，那它们的散列值必须相等，否 则散列表就不能正常运行了。例如，如果 `1 == 1.0` 为真，那么 `hash(1) == hash(1.0)` 也必须为真，但其实这两个数字（整型 和浮点）的内部结构是完全不一样的。
    
    为了让散列值能够胜任散列表索引这一角色，它们必须在索引空间 中尽量分散开来。这意味着在最理想的状况下，越是相似但不相等 的对象，它们散列值的差别应该越大。示例 03-8 是一段代码输出，这段代码被用来比较散列值的二进制表达的不同。注意其中 `1` 和 `1.0` 的散列值是相同的，而 `1.0001`、`1.0002` 和 `1.0003` 的散列值则非常不同。 

    > 示例 03-8 在32 位的 Python 中，1、1.0001、1.0002 和 1.0003 这几个数的散列值的二进制表达对比（上下两个二进制间不同 的位被 ! 高亮出来，表格的最右列显示了有多少位不相同） 
    
    ```py
    32-bit Python build 
    1        00000000000000000000000000000001
                                              != 0 
    1.0      00000000000000000000000000000001 
    ------------------------------------------------
    1.0      00000000000000000000000000000001
               ! !!! ! !! ! !    ! ! !! !!!   != 16 
    1.0001   00101110101101010000101011011101 
    ------------------------------------------------
    1.0001   00101110101101010000101011011101
              !!!  !!!! !!!!!   !!!!! !!  !   != 20 
    1.0002   01011101011010100001010110111001 
    ------------------------------------------------
    1.0002   01011101011010100001010110111001
              ! !   ! !!! ! !  !! ! !  ! !!!! != 17 
    1.0003   00001100000111110010000010010110 
    ------------------------------------------------
    ```
    
    > 从 **Python3.3** 开始，str、bytes 和 datetime 对象的散 列值计算过程中多了随机的“加盐”这一步。所加盐值是 Python 进程内的一个常量，但是每次启动 Python 解释器都会生成一个 不同的盐值。随机盐值的加入是为了防止 DOS 攻击而采取的 一种安全措施。
    
* 散列表算法

    为了获取 `my_dict[search_key]` 背后的值，**Python** 首先会调用 `hash(search_key)` 来计算 `search_key` 的散列值，把这个值最低 的几位数字当作偏移量，在散列表里查找表元（具体取几位，得看当前散列表的大小）。若找到的表元是空的，则抛出 `KeyError` 异常。若不是空的，则表元里会有一对 `found_key:found_value`。 这时候 **Python** 会检验 `search_key == found_key` 是否为真，如果它们相等的话，就会返回 `found_value`。 
    
    如果 `search_key` 和 `found_key` 不匹配的话，这种情况称为散列冲突。发生这种情况是因为，散列表所做的其实是把随机的元素映射到只有几位的数字上，而散列表本身的索引又只依赖于这个数字的一部分。为了解决散列冲突，算法会在散列值中另外再取几位，然后用特殊的方法处理一下，把新得到的数字再当作索引来寻找表元。 若这次找到的表元是空的，则同样抛出 `KeyError`；若非空，或者键匹配，则返回这个值；或者又发现了散列冲突，则重复以上的步骤。
    
    ![ZyHjqx.png](https://s2.ax1x.com/2019/07/09/ZyHjqx.png)
    
    添加新元素和更新现有键值的操作几乎跟上面一样。只不过对于前者，在发现空表元的时候会放入一个新元素；对于后者，在找到相对应的表元后，原表里的值对象会被替换成新值。 
    
    另外在插入新值时，**Python** 可能会按照散列表的拥挤程度来决定是 否要重新分配内存为它扩容。如果增加了散列表的大小，那散列值所占的位数和用作索引的位数都会随之增加，这样做的目的是为了减少发生散列冲突的概率。 
    
    表面上看，这个算法似乎很费事，而实际上就算 dict 里有数百万 个元素，多数的搜索过程中并不会有冲突发生，平均下来每次搜索 可能会有一到两次冲突。在正常情况下，就算是最不走运的键所遇 到的冲突的次数用一只手也能数过来。
    
### dict的实现及其导致的结果 